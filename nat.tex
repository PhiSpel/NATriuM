\documentclass[1p, sort&compress]{elsarticle}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL /mnt/fdrive/tmp/natrium_paper.tex         Tue Oct 18 17:06:59 2016
%DIF ADD /mnt/fdrive/tmp/natrium_paper_stick.tex   Thu Oct 27 19:39:12 2016
\parindent0pt
\usepackage{amsmath,amssymb}
%\usepackage{graphicx}% Include figure files
%\usepackage{caption}
%\usepackage{color}% Include colors for document elements
%\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage[numbers,super,comma,sort&compress]{natbib}
\usepackage{siunitx}
%\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{pdfpages} 
\usepackage{caption}
\usepackage{subcaption} % multi-figures
\usepackage{hyperref}
\usepackage{booktabs} %to enable augmented table formatting, e.g. cmidrule

%\usepackage[nolists, nomarkers, figuresfirst]{endfloat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Added by Andreas Kraemer
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{epstopdf}
% figures side by side
%\usepackage{subfig}
\epstopdfsetup{update} % only regenerate pdf files when eps file is newer
%\newcommand*\samethanks[2][\value{footnote}]
%{\footnotemark[#1]\footnotemark[#2]}
%\usepackage{epsf} 
% For line break in cell:
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
%scientific notation definition
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

%tabular definitions
%\newcolumntype{C}[1]{%            Tabellenzelle linksb\"undig und begrenzte Breite
%>{\raggedright\arraybackslash}p{#1}}
%\newcolumntype{S}[1]{%            Tabellenzelle rechtsb\"undig und begrenzte Breite
%>{\arraybackslash\raggedleft}p{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End Added by Andreas Kraemer
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

% Check at the end
% -- naked this
% -- which / that
% -- 

\begin{frontmatter}

%\title{CoSMoS: Calibration of molecular force fields by Simultaneous Modeling of Simulated data}
\title{NATriuM -- An Extensible Framework for Lattice Boltzmann Simulations on Irregular Grids}

\author[hbrs]{Andreas Kr\"amer}
\ead{kraemer.research@gmail.com, andreas.kraemer@h-brs.de}
\author[hbrs]{Dominik Wilde}
\ead{dominik.wilde@h-brs.de}
\author[hbrs]{Knut K\"ullmer}
\ead{knut.kuellmer@h-brs.de}
\author[hbrs]{Dirk Reith}
\ead{dirk.reith@h-brs.de}
\author[siegen]{Holger Foysi}
\ead{holger.foysi@uni-siegen.de}
\author[hbrs]{Wolfgang Joppich}
\ead{wolfgang.joppich@h-brs.de}

\address[hbrs]{
%Department of Electrical Engineering, Mechanical Engineering and Technical Journalism,
Institute for Technology, Renewables and Energy-efficient Engineering, Bonn-Rhein-Sieg University of Applied Sciences, Grantham-Allee 20, 53757 Sankt Augustin, Germany}
\address[siegen]{Department of Mechanical Engineering, University of Siegen,  Paul-Bonatz-Stra{\ss}e 9-11, 57076 Siegen-Weidenau, Germany}


\begin{abstract}
The lattice Boltzmann method is a modern approach to simulate fluid flow. In its original formulation, it is restricted to regular grids, second-order discretizations, and a unity CFL number. This paper describes our new off-lattice Boltzmann solver NATriuM, an extensible and parallel C++ code to perform lattice Boltzmann simulations on irregular grids. NATriuM also allows high-order spatial discretizations and non-unity CFL numbers to be used. We demonstrate how these features can efficiently decrease the number of grid points required in a simulation and \DIFdelbegin \DIFdel{by these means }\DIFdelend \DIFaddbegin \DIFadd{thus }\DIFaddend reduce the computational time, compared to the standard lattice Boltzmann method. 

\end{abstract}

\begin{keyword}
computational fluid dynamics, unstructured grid, off-lattice Boltzmann, semi-Lagrangian, high-performance computing
%A list of five key words or phrases which best characterize the paper are required for indexing.
\end{keyword}

\end{frontmatter}


\section{Introduction} % Not needed for rapid communications

The Lattice Boltzmann method (LBM) is a modern approach to computational fluid dynamics. While classical fluid flow solvers directly discretize the governing equations of fluid motion, the LBM has emerged out of kinetic theory. Accordingly, it models the dynamics of particle distribution functions due to streaming and collision of particles. This detour into the mesoscopic world is justified in that it simplifies the numerical procedure. Other than classical solvers, the LBM does not have to solve a Poisson equation for the pressure or calculate spatial derivatives. Instead, it alternately (1) streams particle distributions along a fixed set of discrete directions to the neighbor nodes on the lattice and (2) performs collision steps locally at each grid point. First of all, both the streaming step and the collision step are explicit and exceptionally efficient on lattices, i.e. regular computation grids. Furthermore, the collision model can be tailored to give the correct physics on the continuum scale and may even be used to include microscopic interactions that exceed the scope of continuum-based simulation.

On the one hand, these properties suit the simulation of complex flows on highly parallel computers. Prominent examples are flows through porous media \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Martys.1996}}%DIFAUXCMD
}\DIFaddend , flows with multiple phases and constituents \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Li.2016}}%DIFAUXCMD
}\DIFaddend , and turbulent flows \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Jahanshaloo.2013}}%DIFAUXCMD
}\DIFaddend . On the other hand, they limit the geometrical and algorithmic flexibility of the method. The geometrical restriction to regular computation grids is the method's most unsatisfactory property. Most importantly, it retards the LBM to be applied to complex wall-bound flows\DIFaddbegin \DIFadd{, }\DIFaddend many of which are ubiquitous in science and engineering. Consequently, generalizations of the LBM were developed to extend \DIFdelbegin \DIFdel{the LBM's }\DIFdelend \DIFaddbegin \DIFadd{its }\DIFaddend domain of applicability. Besides hierarchical grid refinements \DIFdelbegin \DIFdel{like }\DIFdelend \DIFaddbegin \DIFadd{such as }\DIFaddend the one by Filippova and H{\"a}nel \cite{Filippova.1998}, many authors combined the LBM concept with classical discretization schemes. They developed various finite-difference LBMs \cite{Fakhari.2015,Hejranfar.2014}, finite-volume LBMs \cite{Patil.2009,StefanoUbertini.2008}, finite-element LBMs \cite{Duster.2006,Li.2005}, and discontinuous-Galerkin LBMs \cite{Zadehgol.2014,Min.2011}.

In contrast to the standard LBM, these off-lattice Boltzmann methods (OLBMs) decouple the discretization of the particle velocity space and the physical space. The particle velocities are modeled using the customary LBM stencils (D2Q7, D2Q9, D3Q15, D3Q19, D3Q27). This discretization leads to the discrete Boltzmann equation, which can be solved with an appropriately chosen numerical scheme on an irregular grid. The operator splitting into streaming and collision \DIFdelbegin \DIFdel{step }\DIFdelend \DIFaddbegin \DIFadd{steps }\DIFaddend can also be formulated in general terms \cite{Bardow.2006}, introducing the well-known $0.5$-shift into the relaxation time (cf. \ref{sec:Appendix}) and rendering the OLBM schemes stable at high Reynolds numbers. With these preliminaries, \DIFdelbegin \DIFdel{they have overcome }\DIFdelend the teething troubles of the first finite volume LBMs \cite{Nannelli.1992} \DIFaddbegin \DIFadd{have been overcome}\DIFaddend .

The number of published OLBMs is growing steadily. They extend the standard LBM by several appealing properties. \DIFdelbegin \DIFdel{First of all, they }\DIFdelend \DIFaddbegin \DIFadd{They }\DIFaddend support simulations on unstructured grids and higher-order spatial convergence  \cite{Min.2011}. \DIFdelbegin \DIFdel{OLBMs }\DIFdelend \DIFaddbegin \DIFadd{They }\DIFaddend have already proven that they can reduce the number of required grid points remarkably, when compared to the standard LBM \cite{Zadehgol.2014}. Their temporal and spatial error can be controlled independently by dropping the \DIFdelbegin \DIFdel{unity-CFL }\DIFdelend \DIFaddbegin \DIFadd{unity-Courant-Friedrichs-Lewy (CFL) }\DIFaddend number restriction; and finally, they can operate with larger time steps via either implicit time integration \cite{Lee.2003} or a recently developed semi-Lagrangian streaming step \cite{Kramer.2016c}.

Despite \DIFdelbegin \DIFdel{of }\DIFdelend these promising properties, most OLBMs have only been applied to simple test problems in two dimensions. Simulations in three dimensions, performance studies, parallel implementations, and comparisons to other CFD methods remain rare\DIFdelbegin \DIFdel{to this date}\DIFdelend . The future prospects of OLBMs will depend on their success \DIFdelbegin \DIFdel{, }\DIFdelend when being applied to complex flows in three dimensions. However, while there \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend an increasing number of publications on standard LBM codes, a comprehensive and extensible tool for OLBM simulations \DIFdelbegin \DIFdel{has not }\DIFdelend \DIFaddbegin \DIFadd{does not appear to have }\DIFaddend been published, \DIFdelbegin \DIFdel{to the knowledge of the authors}\DIFdelend \DIFaddbegin \DIFadd{yet}\DIFaddend .

The present paper attempts to fill this gap. We introduce our extensible and parallel OLBM solver NATriuM. NATriuM \DIFdelbegin \DIFdel{is a }\emph{\DIFdel{C++}} %DIFAUXCMD
\DIFdel{code that is }\DIFdelend \DIFaddbegin \DIFadd{was first developed to perform }\textbf{\DIFadd{N}}\DIFadd{umerical simulations for }\textbf{\DIFadd{A}}\DIFadd{pplications in }\textbf{\DIFadd{Tri}}\DIFadd{bology }\textbf{\DIFadd{u}}\DIFadd{sing }\textbf{\DIFadd{M}}\DIFadd{ulticore processors. It developed from the original idea into a multi-purpose flow solver written in C++ and }\DIFaddend based on the finite element library deal.II \cite{Bangerth.2007}. It uses deal.II's quadrilateral grid structure and interface to the parallel environment Trilinos \cite{MichaelHeroux.2003} and the octree library p4est \cite{Burstedde.2011}. The code is modular and follows a dimension-independent programming approach, which makes it easy to include new spatial discretization schemes, time integrators, boundary conditions, particle velocity stencils, and collision models. The complete setup makes NATriuM an ideal tool for exploring the potential of OLBMs.

%\subsection{Structure of the article}
This article proceeds as follows.
In the Methodology section, the LBM and its generalization to unstructured grids are briefly summarized. 
%We shortly summarize the solution of the advection equation that was previously described by Min and Lee \cite{Min.2011}. Second, the implementation is described in detail. 
Then, the most important features of the presented tool are consecutively described\DIFdelbegin \DIFdel{and demonstrated }\DIFdelend \DIFaddbegin \DIFadd{, demonstrated and discussed}\DIFaddend . These features are the efficient utilization of third-party libraries, the modular code structure, the high-order convergent spatial discretization, the use of arbitrary quadrilateral and hexagonal grids, and the dimension-independent programming concept. \DIFaddbegin \DIFadd{Each of the Sections \ref{sec:TPL} -- \ref{sec:dimension-independent} addresses one of these features and divides into two subsections ``Description'' and ``Results and Discussion''. }\DIFaddend The simulations within this manuscript include a performance study, a convergence test, shear flows in sinusoidal geometries, and a three-dimensional turbulent channel flow on an irregular grid.

\section{Methodology}
\subsection{LBM}
The LBM solves a discrete form of the Boltzmann equation
\begin{equation}
	\label{eq:DBE}
	\frac{Df}{Dt} = \Omega(f),
\end{equation}
where $f$ denotes the particle distribution function. While the total derivative on the \DIFdelbegin \DIFdel{left hand }\DIFdelend \DIFaddbegin \DIFadd{left-hand }\DIFaddend side accounts for free-flight (advection) of particles, the term on the \DIFdelbegin \DIFdel{right hand }\DIFdelend \DIFaddbegin \DIFadd{right-hand }\DIFaddend side accounts for particle collisions. The concrete collision term $\Omega$ determines the hydrodynamics and can take various forms.

The LBM discretizes the particle-velocity space into a set of discrete velocities $e_0,\dots,e_{Q-1},$ resulting in a vector of particle distribution functions $f=(f_0,\dots,f_{Q-1})$ and a discrete collision operator $\Omega=(\Omega_{0},\dots,\Omega_{Q-1}),\ Q \in \mathbb{N}.$
The most important particle-velocity stencils are the D2Q9 \DIFdelbegin \DIFdel{stencil }\DIFdelend (dimension $D=2$, number of velocities $Q=9$)\DIFdelbegin \DIFdel{and the }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend D3Q15, D3Q19, and D3Q27 stencils.

Equation \eqref{eq:DBE} can be split into a collision and an streaming step, cf. \ref{sec:Appendix},
\begin{align}
	\label{eq:collision}
	&\tilde{f}_{\alpha} (x,t) = f_{\alpha} (x,t) + \Omega_{\alpha} \left( f(x,t) \right) 
		\quad \mathrm{and}\\
	\label{eq:streaming}
	&f_{\alpha} ( x,t ) = \tilde{f}_{\alpha} (x - \delta_t e_\alpha, t-\delta_t),
		\quad \mathrm{respectively.}
\end{align}
This splitting introduces a second-order temporal error \DIFdelbegin \DIFdel{, }\DIFdelend but greatly facilitates the numerical procedure. Most importantly, it renders the collision step (i.e. Equation \eqref{eq:collision}) local in space and time. Notably, the splitting does not affect the spatial accuracy of the scheme, which offers the potential for high-order spatial convergence. This property follows from the theoretical derivation but is also demonstrated later for an exemplary flow.

\subsection{Advection Operator}
On regular grids, the advection step is exact as $x-\delta_t e_{\alpha}$ represents a \DIFdelbegin \DIFdel{neighbor }\DIFdelend \DIFaddbegin \DIFadd{neighboring }\DIFaddend node on the lattice; the advection reduces to a simple index shift. On irregular grids, the streaming step has to be replaced by a more sophisticated procedure. 

\DIFdelbegin \DIFdel{Some previous works }\DIFdelend \DIFaddbegin \DIFadd{Previous work }\DIFaddend \cite{Bardow.2006,Min.2011} utilized that Equation \eqref{eq:streaming} \DIFdelbegin \DIFdel{equates }\DIFdelend \DIFaddbegin \DIFadd{is equivalent to }\DIFaddend solving the initial value problem 
\begin{equation}
	\frac{Df}{Dt} = 0, \quad f_{\alpha}(t_{i-1}) = \tilde{f}_\alpha(t_{i-1}),
	\label{eq:advection}
\end{equation}
for $t_{i}$, where $t_{i-1}$ denotes the previous time step. 
Among the various numerical approaches to this linear advection equation, NATriuM implements the one by Min and Lee \cite{Min.2011} in using their spectral-element discontinuous Galerkin scheme.

As an alternative to solving \eqref{eq:advection}, we have recently developed a semi-Lagrangian streaming step \cite{Kramer.2016c}\DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{, cf. \ref{sec:SL}. }\DIFaddend Our new approach uses a semi-Lagrangian method to approximately solve Equation \eqref{eq:streaming}, interpolating the distribution functions at the departure points $x-\delta_t e_\alpha$ by finite elements. This procedure circumvents \DIFdelbegin \DIFdel{to use }\DIFdelend \DIFaddbegin \DIFadd{using }\DIFaddend a time integrator, which advantageously eliminates the pervasive CFL constraint. Moreover, it requires only one evaluation of the spatial operator per time step. Those appealing properties of the semi-Lagrangian streaming step are discussed in detail in a separate publication \cite{Kramer.2016c}.

%
%
%Its starting point is the weak formulation of the linear advection equation with a Lax-Friedrichs flux
%\begin{equation}
%\left( \varphi,\ \frac{\partial f_{\alpha}}{\partial t} + e_{\alpha i} \frac{\partial f_{\alpha}}{\partial x_i} \right)_{\Omega^E}
% = \left( \varphi,\ \frac{1}{2} (n_{i} e_{\alpha i} - | n_{i} e_{\alpha i} |)(f_{\alpha}-f_{\alpha}^{+}) \right)_{\partial \Omega^E}
% \label{eq:AE_weak}
%\end{equation}
%on each cell $\Omega^E$ of the unstructured grid.
%Here, $n$ and $f_{\alpha}^{+}$ denote the cell normals and solution on the neighbor cell, respectively.  At the boundaries, $f_{\alpha}^{+}$ is constructed according to boundary conditions, which are specified later.
% 
%The shape and basis functions are constructed on a reference cell $\tilde{\Omega}:=[0,1]^D$ with the $p$-th-order $D$-dimensional Legendre-Lagrange polynomials $\tilde{\varphi}^{(j)}$ on the Gau{\ss}-Lobatto-Legendre points $\tilde{x}^{(j)}, j=1,\dots,(p+1)^D.$ Consequently, $\tilde{\varphi}^{(j)}(\tilde{x}^{(k)}) = \delta_{jk}$ and $(\tilde{\varphi}^{(j)},\tilde{\varphi}^{(k)})_{\tilde{\Omega}} = \delta_{jk}.$ A bilinear mapping $\tilde{M}^{E}:\tilde{\Omega} \rightarrow \Omega^{E}$ is used to map the reference cell to the real cell, resulting in a discretization
%\begin{equation*}
%f_{\alpha}(x,t) \approx f_{\alpha}^{(E,k)} \tilde{\varphi}^{k}\left( (\tilde{M}^{E})^{-1}(x) \right)
%\end{equation*}
%(summation over $j$ implied) on each local cell, where $f_{\alpha}^{(E,k)}:=f_{\alpha}(\tilde{M}^{E}(\tilde{x}^{(k)})).$
%Substituting into the weak form \eqref{eq:AE_weak} yields a system of linear ODEs
%\begin{equation*}
%m_{\alpha}^{(E,jk)} \frac{\partial f_{\alpha}^{(E,k)}}{\partial t} = 
%-e_{\alpha i}  s_{\alpha}^{(E,ijk)} f_{\alpha}^{(E,k)} 
%+ r_{\alpha}^{(E,jk)} \left(  f_{\alpha}^{(E,k)} -  {f_{\alpha}^{(E,k)}}^{+} \right),
%\end{equation*}
%where the mass, stiffness and boundary matrices 
%\begin{align*} 
%m_{\alpha}^{(E,jk)} 	&= 	\left( \tilde{\varphi}^{j} \circ (\tilde{M}^{E})^{-1},  
%					\tilde{\varphi}^{k} \circ (\tilde{M}^{E})^{-1} \right)_{\Omega^E} ,\\
%s_{\alpha}^{(E,ijk)} 	&= \left( \tilde{\varphi}^{j} \circ (\tilde{M}^{E})^{-1},  
%					\frac{ \partial \left[ \tilde{\varphi}^{k} \circ (\tilde{M}^{E})^{-1} \right] }{\partial x_i} \right)_{\Omega^E} ,\\
%r_{\alpha}^{(E,jk)}		&=\left( \tilde{\varphi}^{j} \circ (\tilde{M}^{E})^{-1},  
%					\tilde{\varphi}^{k} \circ (\tilde{M}^{E})^{-1} \right)_{\partial\Omega^E},
%\end{align*}
%are computed with the transformation rule and the Gau{\ss}-Lobatto-Lagrange quadrature.
%This makes the mass matrix diagonal, giving an explicit system of ODEs
%\begin{equation}
% \frac{\partial f}{\partial t} (t) = L f + d(f),
%\label{eq:ODE}
%\end{equation}
%which can be solved by virtually any time integration method. The function $d(f)$ depends on the boundary conditions.
%
%Three points remain to be covered: collision models, boundary conditions, and time integrators.

\subsection{Collision Model}
The form of the collision operator $\Omega$ determines the hydrodynamic equations that are solved by the lattice Boltzmann simulation. The most important models are single-relaxation time \cite{McNamara.1988}, multiple-relaxation time \cite{Lallemand.2000}, entropic models \cite{Karlin.1999} and their respective extensions. We restrict the description in this subsection to the widely-used \DIFdelbegin \DIFdel{Bhatnagar-Gro}%DIFDELCMD < {%%%
\DIFdel{\ss}%DIFDELCMD < }%%%
\DIFdel{-Krook }\DIFdelend \DIFaddbegin \DIFadd{Bhatnagar-Gross-Krook }\DIFaddend (BGK) model \cite{Bhatnagar.1954}. 
%, a preconditioned model for steady flows by Guo et al. [], and the recently proposed entropic multiple-relaxation time model by Karlin, Boesch and Chikatamarla (KBC) [].

The BGK model is defined as $\Omega(f) = - \frac{1}{\tau} (f-f^{\mathrm{eq}})$ with 
\begin{equation*}
f_{\alpha}^{\mathrm{eq}} = w_{\alpha} \rho \left(1 + \frac{u_i e_{\alpha i}}{c_{S}^{2}} - \frac{u_i u_i}{2 c_{S}^2} + \frac{u_i e_{\alpha i} u_j e_{\alpha j}}{2 c_{S}^4} \right)
\end{equation*}
\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{(Einstein summation over $i$ and $j$ implied) and }\DIFaddend $ \tau = \frac{\nu}{c_{S}^{2} \delta_t} + 0.5.$ Here, $\nu,$ $c_S,$ and $w_{\alpha}$ denote the kinematic viscosity, speed of sound, and lattice-specific weights, respectively. The conservative moments, $\rho := \sum_{\alpha} f_{\alpha}$ and $\rho u := \sum_{\alpha} f_{\alpha} e_{\alpha}$, recover the compressible Navier-Stokes equations in the weakly compressible limit \DIFaddbegin \DIFadd{for Mach number $Ma \ll 1$}\DIFaddend .

\subsection*{}
The unstructured streaming step and the collision step are basic ingredients of the off-lattice Boltzmann approach. They are supplemented by appropriate boundary conditions for the distribution functions and time integrators for the advection equation \eqref{eq:advection}. Each of these components can take various \DIFdelbegin \DIFdel{concrete }\DIFdelend forms that are surveyed in more detail as the paper proceeds. The following sections explain the most important aspects of the off-lattice Boltzmann code NATriuM, starting with a description of the incorporated third-party libraries.

%The steady-state preconditioned model is defined as the BGK model, but with
%\begin{equation*}
%f_{\alpha}^{\mathrm{eq}} = w_{\alpha} \rho \left(1 + \frac{u_i e_{\alpha i}}{c_{S}^{2}} - \frac{u_i u_i}{2 \gamma c_{S}^2} + \frac{u_i e_{\alpha i} u_j e_{\alpha j}}{2 \gamma c_{S}^4} \right),
%\end{equation*}
%where $\gamma$ is a user-defined preconditioning parameter. Adjusting $\gamma$ accelerates the convergence towards a steady state, in which the conservative moments approximately solve the Navier-Stokes equations.
%
%The KBC model defines $\Omega(f) = -\beta f + \beta f^{\mathrm{mirr}}$
%with $\beta = \frac{c_{S}^2}{c_{S}^2 + 2\nu}$ and 
%\begin{equation*}
%f^{\mathrm{mirr}} = k + (2s-s^{\mathrm{eq}}) + \big( \tilde{\gamma}h + (1-\tilde{\gamma}) h^{\mathrm{eq}} \big).
%\end{equation*}
%Here, $k$, $s$, and $h$ denote the conservative, shear and higher-order moments of $f,$ respectively; $s^{\mathrm{eq}}$, and $h^{\mathrm{eq}}$ denote the shear and higher-order moments of $f^{\mathrm{eq}},$ respectively; and $\tilde{\gamma}$ is a local parameter that does not affect the hydrodynamics. The latter automatically adapts to the flow so as to maximize the entropy of the post-collision distribution $S(f+\Omega(f))$.
%It has been shown that this adaptive relaxation of high-order moments stabilizes the simulation, especially when it comes to under-resolved turbulence.

%
%\subsection{Boundary conditions}
%
%%At solid boundaries, $f_{\alpha}^{+}$ is constructed by bounce-back of non-equilibrium parts with respect to the prescribed density $\rho_0$ and velocity $u_{b}:$
%%\begin{equation*}
%%f_{\alpha}^{+} = f_{\alpha^{\ast}} - 2 w_{\alpha} \rho_0 \frac{e_{\alpha i} u_i}{c_{S}^2},
%%\end{equation*}
%%where $\alpha^{\ast}$ denotes the opposite index with $e_{\alpha^{\ast}} = -e_{\alpha}$.
%
%In the LBM, at least the incoming distributions at the boundary nodes have to be set to close the problem. They are usually calculated from the local distribution functions as well as the prescribed pressure and velocity as functions $f_{\alpha}^{+}(f, \rho_w, u_w)$. There are three different types of boundaries that are handled differently:
%\begin{itemize}
%\item Periodic boundaries: $f_{\alpha}^{+}$ is replaced by the $f_{\alpha}$ from the other side of the computational domain. Consequently, at periodic boundaries, $d(f)=0$.
%\item Linear boundaries: A common choice is the bounce back of non-equilibrium distributions
%\begin{equation*}
%f_{\alpha}^{+} - f_{\alpha}^{\mathrm{eq}} = f_{\alpha^{\ast}} - f_{\alpha^{\ast}}^{\mathrm{eq}},
%\end{equation*}
%where $\alpha^{\ast}$ denotes the opposite index with $e_{\alpha^{\ast}} = -e_{\alpha}$.
%When the equilibrium distribution is evaluated for a prescribed density and velocity,
%the above equation becomes linear
%\begin{equation*}
%f_{\alpha}^{+} = f_{\alpha^{\ast}} + 2 w_{\alpha} \rho_w \frac{e_{\alpha i} u_i}{c_{S}^2}.
%\end{equation*}
%The second term on the right hand side is incorporated into $d(f),$ which becomes constant in time.
%\item Nonlinear boundaries: Whenever $f_{\alpha}^{+}$ depends nonlinearly on the other distribution functions at the boundary, $d(f)$ becomes nonlinear, which may complicate the time integration.
%\end{itemize}

%\subsection{Time integrators}
%
%The ODE system \eqref{eq:ODE} can be solved with various time integration schemes. At this stage, NATriuM supports different explicit, implicit, and embedded Runge-Kutta methods as well as an exponential integrator. Since the Runge-Kutta methods are well-known, we restrict this description to the exponential integrator.
%a singly-diagonally-implicit Runge-Kutta method.

%The Runge-Kutta method was also used in the original SEDG-LBM paper. It reads
%\begin{align*}
%	g_0 &:= f(t),\\
%	g_\kappa &:= \sum_{\iota = 0}^{\kappa-1} a_{\kappa \iota} g_{\iota} 
%		+ b_{\kappa \iota} \delta_t   \left( L g_{\iota} +  d (g_{\iota}) \right) 
%		,\quad \kappa=1,\dots,5 ,\\
%	f(t+\delta_t) &:=g_5,
%\end{align*}
%where $a_{\kappa \iota}$ and $b_{\kappa \iota}$
%% and $c_{\iota}$ 
%%are given constants \cite{Hesthaven.2008}.
%A similar integrator was already introduced in \cite{Uga.2013}; yet, our implementation uses Padé approximations and can be used for linear ODEs systems with constant $d$.
%Hence, the exponential integrator is only valid when no nonlinear boundaries are present. In this case, an optimized approximation of the solution of equation (\ref{eq:ODE}) can be described as
%\begin{equation}
%f(t+\delta t) \approx f(t) + \delta t \|f(t)\|_2 V_{m+1} \varphi (\delta t \overline{H}_{m+1})e_1
%\end{equation}
%with the extended matrix exponential
%\begin{equation*}
%\varphi_1(\delta t \overline{H}_{m+1})  = \begin{pmatrix}
%\varphi_1(\delta t H_m) & 0 \\ h_{m+1} \delta t e_m \varphi_2(\delta t H_m ) & 1 
%\end{pmatrix}
%\end{equation*}
%and the relation
%\begin{equation*}
%\varphi_0(z)=e^z, \varphi_j(z) = \varphi_{j-1}(z)-\varphi_{j-1}(0))/z.
%\end{equation*}
%An orthonormal basis $V_{m+1}$ of $L$ and the corresponding upper Hessenberg matrices $H_{m+1}$ and $H_m$ are computed by the Arnoldi process, whereas an efficient implementation of the matrix exponential is given by the Padé approximation. $e_m$ is the m-th basis unit vector \cite{Sidje.1998}.


\section{Third-Party Libraries}
\DIFaddbegin \label{sec:TPL}
\DIFaddend \subsection{Description}
NATriuM uses four different third-party libraries. It is largely based on deal.II, a comprehensive, award-winning finite element library. 
%that has won the Wilkinson Price for Numerical Software in 2007
Deal.II implements the basic components of finite element and discontinuous Galerkin methods, such as computational grids, shape functions, numerical quadratures, sparse matrices, etc. It also bases a lot of its functionality on other libraries, among which NATriuM uses boost, p4est \cite{Burstedde.2011}, and Trilinos \cite{MichaelHeroux.2003}.

Boost is \DIFdelbegin \DIFdel{necessarily }\DIFdelend required from deal.II. NATriuM also uses its unit test framework to maintain the code quality at each stage of the developing process. 

P4est accounts for parallel partitioning of the mesh. It employs distributed octrees to balance the workload among processors on distributed memory, scaling up to thousands of cores. Quadrilateral (2D) and hexagonal (3D) meshes are supported. 

Trilinos is a collection of packages for large-scale scientific computing. Most importantly, it offers linear algebra functionality on distributed memory. All large-scale matrices and vectors in NATriuM are built on Trilinos’ data structures, \DIFdelbegin \DIFdel{which ensures }\DIFdelend \DIFaddbegin \DIFadd{ensuring }\DIFaddend efficient and parallel matrix-vector operations. This efficiency is crucial \DIFdelbegin \DIFdel{, }\DIFdelend since the linear algebra operations usually make up the major part of NATriuM’s runtime.


\subsection{Results and Discussion}
To demonstrate the benefits of using highly parallel third-party libraries, the parallel performance is evaluated for simulations of a three-dimensional Couette flow. The flow domain $\tilde{D}:=[0,L]\times[0,1]\times[0,1]$ was discretized into cells of equal size, with periodic boundaries along the $x$- and $y$-directions and solid walls at $z=0$ and $z=1$, cf. \cite{Min.2011} for the wall boundary condition. The flow was simulated with BGK collisions and a discontinuous Galerkin streaming with classical Runge-Kutta time stepping, as described in \cite{Min.2011}. The order of finite elements was set to $p=1$. The problem size was varied by the refinement level $N$ and the length of the domain, resulting in a total of $N_{\mathrm{points}} = (p+1)^3 \cdot 2^{3N} \cdot L$ grid points, i.e. the number of quadrature nodes per cell times the number of cells.  Each compute node consisted of two Intel Xeon X5650 six-core processors and 48 GB RAM. One MPI process was assigned to each core. 
%As a performance measure we counted the number of grid point updates per second, which include collision and streaming operations. 

\begin{figure}[htbp]
 	\centering
	%\includegraphics[width=0.49\linewidth]{one_node_various_p.png}
	\includegraphics[width=0.49\linewidth]{figures/scaling.pdf}
 	\caption{\label{fig:speedup} Weak and strong scaling on up to 64 nodes, compared to an ideal linear speedup. The number of cells per node and number of total cells were $131\,072$ and $1\,048\,576$ for the weak and strong scaling, respectively.}
\end{figure}

%Results
%First, the performance on a single node was studied for various orders of finite elements and numbers of cells, concretely $p = 1,2,3,4,5;\ $ $N=2,3,4,5;\ $ $L=1,4,8,16,32,64$. 
%Figure \ref{fig:performance} (left) shows the number of grid point updates per second versus the number of grid points. The rightmost point for each $p$ indicates the largest problem that fit into the memory. 
%TODO How many grid points fit into one GB of RAM?
%For $p=1$ it was $4.2\cdot 10^{6}$ grid points, i.e. approximately $10^5$ grid points per GB. As $p$ increased, the maximum attainable problem size and the computational performance dropped. A performance peak appears when the workload is two orders of magnitude lower than the maximum problem size. This peak flattens, as $p$ grows. 

The parallel performance was studied for a fixed problem size per node (weak scaling) and overall fixed problem size (strong scaling). Figure \ref{fig:speedup} shows the speedup. The simulations for the weak scaling had $N=5$ and $L=4\cdot{\#}\mathrm{Nodes}.$ Varying the number of nodes from one to 64, a close-to-ideal speedup was obtained. On 64 nodes, the parallel efficiency was still $84{\%}.$ The simulations for the strong scaling had $N=5$ and $L=32$. From two to 24 nodes, a super-linear speedup was observed. On 64 nodes, the parallel efficiency was still $90{\%}.$
%OUTLOOK: Save memory

The scalability of NATriuM is due to the interplay of deal.II, Trilinos and p4est. The combination of these three has been shown to scale well to more than $16\, 000$ processors \cite{Bangerth.2011}. Their incorporation in NATriuM facilitates quickly writing highly parallel code.

\section{Modular Code Structure}
\subsection{Description}
\DIFaddbegin \begin{figure}[thbp]
	\includegraphics[width=\linewidth]{figures/uml.pdf}
	\caption{\DIFaddFL{NATriuM's modular concept. The class }\texttt{\DIFaddFL{CFDSolver<dim>}} \DIFaddFL{manages the interplay of the }\texttt{\DIFaddFL{Stencil}}\DIFaddFL{, }\texttt{\DIFaddFL{AdvectionOperator<dim>}}\DIFaddFL{, and }\texttt{\DIFaddFL{CollisionModel}} \DIFaddFL{to simulate a flow that is defined in the }\texttt{\DIFaddFL{ProblemDescription<dim>.}}  \DIFaddFL{The latter owns an instance of }\texttt{\DIFaddFL{BoundaryCollection<dim>}} \DIFaddFL{that defines each boundary conditions as a }\texttt{\DIFaddFL{Boundary<dim>}} \DIFaddFL{object. The }\texttt{\DIFaddFL{AdvectionOperator<dim>}} \DIFaddFL{performs the streaming step, which can be done using a }\texttt{\DIFaddFL{TimeIntegrator}}\DIFaddFL{. Italic text indicates abstract classes that are specialized by other classes, e.g., }\texttt{\DIFaddFL{Stencil}} \DIFaddFL{is specialized by  }\texttt{\DIFaddFL{D2Q9}}\DIFaddFL{, }\texttt{\DIFaddFL{D3Q15}}\DIFaddFL{, }\texttt{\DIFaddFL{D3Q19}}\DIFaddFL{, and }\texttt{\DIFaddFL{D3Q27}}\DIFaddFL{.  }\label{fig:uml}}
\end{figure}
\DIFaddend %\begin{figure}[htbp]
% 	\centering 	
%	\includegraphics[trim={2cm 2cm 2cm 2cm},clip,width=0.8\textwidth]{class_diagram_15_12_01.pdf}
% 	\caption{\label{fig:class_diagram} Class diagram of the NATriuM software: 
% 	The central solver module depends on certain other modules: 
% 	a problem description module, which stores the problem geometry and boundary conditions,
% 	an advection operator for the assembly of the streaming matrix (spatial discretization of the advection equation),
% 	a time integrator for the solution of the advection equation (temporal discretization),
% 	a collision module for different types of collisions,
% 	a module to store the Boltzmann models that contain the equilibrium distributions,
% 	and a utilities module.
% 	The \emph{CFDSolver} is the central class and comes with a container class for the particle distribution functions, 
% 	another one to configure the solver preferences
% 	and one to calculate macroscopic variables such as kinetic energy. 
% 	There are two additional classes BenchmarkSolver and ErrorStats for Benchmark problems,
% 	that have an analytical solution.
% 	 The \emph{ProblemDescription} class is abstract and can be overriden by concrete flow problems, which have to define
% 	the grid (m\_triangulation), initial values and store the boundary conditions in a container BoundaryCollection.
% 	The \emph{AdvectionOperator} class is also abstract and currently only realized through a spectral element
% 	discontinuous Galerkin solver.
% 	There are various time integrators implemented through the virtual step function of the abstract class
% 	\emph{TimeIntegrator}.
%	The \emph{CollisionModel} class serves as a common parent class for all specializations of the Collision template.
% 	This architecture facilitates to inline the functions called in the collision step.
% 	Most of the modules make use of the abstract \emph{BoltzmannModel} class which is overriden by various $2$D and $3$D fluid flow models. }
%\end{figure}
NATriuM is composed as \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend object-oriented library, allowing for easy extensions through a modular program structure\DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{, cf. Fig. \ref{fig:uml}. }\DIFaddend The two central classes are called \texttt{ProblemDescription<dim>} and \texttt{CFDSolver<dim>}. While the problem description contains the computational grid and all relevant information about the flow, the CFD solver steers the simulation. This separation between flow description and simulation facilitates both using different solver configurations for the same flow and easily setting up new flows. These two basic components of NATriuM are described in more detail in the following.

\subsubsection{Implementation of the flow definition}
The definition of the flow includes the relevant physical fluid measures (i.e., the kinematic viscosity for incompressible, isothermal flow), the mesh, the initial and boundary conditions\DIFdelbegin \DIFdel{as well as }\DIFdelend \DIFaddbegin \DIFadd{, and }\DIFaddend external forces. The class \texttt{ProblemDescription<dim>} serves as a container for this information. Each concrete flow can \DIFdelbegin \DIFdel{either be }\DIFdelend \DIFaddbegin \DIFadd{be either }\DIFaddend implemented as a subclass of \texttt{ProblemDescription<dim>} or read in from a file.% via the class \texttt{ProblemDescriptionFromFile<dim>}.
% When choosing the first option, the file is processed by the class \texttt{FlowFromFile} that is also a 

The boundary conditions are defined in a \DIFdelbegin \DIFdel{so called }\DIFdelend \DIFaddbegin \DIFadd{so-called }\DIFaddend \texttt{BoundaryCollection<dim>} that handles three different types of boundary conditions: Periodic boundaries, linear flux boundaries\DIFaddbegin \DIFadd{, }\DIFaddend and ``standard LBM boundaries''. They are passed to the boundary collection in an analogous manner but their effect on the code differs greatly. 
Periodic boundaries contain a list of cell and face pairs that are neighbors across the boundary. 
%This list has to be filled before the mesh gets refined. Consequently, the mesh refinement has to be done after the boundary definition. Upon assembly of the linear system, the periodic faces are treated in the same way as internal faces. 
Linear boundaries are incorporated into the sparse matrix and a constant system vector (upon assembly of the spatial operator)\DIFaddbegin \DIFadd{, }\DIFaddend and ``standard LBM boundaries'' are processed separately in each iteration, as in the standard LBM.
%With explicit integrators, they have to be reassembled in each Runge-Kutta step. In the current version of NATriuM, implicit and exponential integrators cannot be used with nonlinear boundaries. To extend the code in this direction, the Jacobians would have to be provided along with each nonlinear boundary to solve the nonlinear system (eq) or provide a linear approximation to ... in equation (eq), respectively. 
Currently, NATriuM supports only four types of boundary conditions: periodic boundaries, velocity bounce-back boundaries for the discontinuous Galerkin advection (cf. \cite{Min.2011}), Grad's boundaries for velocities (cf. \cite{Dorschner.2015}), and Grad's boundaries for pressure.

Before the simulation starts, the problem description is passed to the CFD solver.



\subsubsection{Implementation of the flow solver}
The \texttt{CFDSolver<dim>} class coordinates the simulation. The solver’s configuration is defined in a separate class \texttt{SolverConfiguration} that has to be specified either in a configuration file or through setter functions. Upon construction, the \texttt{CFDSolver<dim>} sets up all components of the simulation code according to the solver configuration and manages their interplay. The pivotal components are the abstract classes \texttt{Stencil}, \texttt{CollisionModel}, \texttt{AdvectionOperator<dim>} and \texttt{TimeIntegrator}.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The \texttt{Stencil} contains the set of discrete velocities, weights, and a moment matrix. Currently, NATriuM \DIFdelbegin \DIFdel{support }\DIFdelend \DIFaddbegin \DIFadd{supports }\DIFaddend the most important velocity stencils: D2Q9, D3Q15, D3Q19, and D3Q27.

The collision model performs the collision step and\DIFaddbegin \DIFadd{, }\DIFaddend in doing so\DIFaddbegin \DIFadd{, }\DIFaddend determines the physics of the simulation. Each collision model is derived from the abstract class \texttt{CollisionModel} and has to implement its virtual method \DIFdelbegin \DIFdel{collideAll}\DIFdelend \DIFaddbegin \texttt{\DIFadd{collideAll()}}\DIFaddend . Most collision models can be formulated generally so that they are valid for all stencils. Yet, it has often proven beneficial to provide optimized stencil-specific codes to speed up the collision step. In the current version, NATriuM supports the BGK scheme, a recent entropic multiple-relaxation-time scheme by Karlin et al. (cf. \cite{Bosch.2015}), a preconditioned scheme for steady simulations \cite{Guo.2004}, and a newly developed pseudo-entropic model \cite{Kramer.2016d}.

The advection operator performs the spatial discretization for the streaming step on an arbitrary quadrilateral or hexagonal grid. Currently, two different advection operators are implemented: the discontinuous Galerkin scheme described in \cite{Min.2011} and the recently developed semi-Lagrangian streaming step \cite{Kramer.2016c}. The time integrator performs the temporal discretization. NATriuM supports various explicit, implicit, and embedded Runge-Kutta time stepping as well as an exponential integrator.
The modular structure of NATriuM allows \DIFdelbegin \DIFdel{to easily incorporate }\DIFdelend \DIFaddbegin \DIFadd{easy incorporation of }\DIFaddend new stencils, collision models, advection schemes and time integrators.


\subsection{Results and Discussion}
Several examples could appear here to demonstrate the use of interchangeable modules within NATriuM. The most important \DIFdelbegin \DIFdel{choice the user has to make is on }\DIFdelend \DIFaddbegin \DIFadd{user choice is }\DIFaddend which advection operator \DIFdelbegin \DIFdel{he wants to use. Therefor }\DIFdelend \DIFaddbegin \DIFadd{should be used. Therefore }\DIFaddend we present a performance comparison of the two implemented advection operators. It is clear that both the discontinuous Galerkin and the semi-Lagrangian advection step complicate the streaming, compared to the standard LBM. In the following study, both schemes are compared to Palabos\DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Palabos}}%DIFAUXCMD
}\DIFaddend , a well-known standard LBM library.

The performance of LBM codes is usually measured in million lattice updates per second (MLUPS). Although NATriuM does not operate on lattices, we still use the same measure, replacing the number of lattice nodes by the number of cells times the number of grid points owned by each cell.

The simulations for this comparison operated on a D3Q19 stencil with a BGK collision scheme. The discontinuous Galerkin advection was performed with different time integrators, most importantly a classical fourth-order Runge-Kutta method. All simulations used MPI for the parallelization and ran on two Intel Xeon E5-2680v3 with 128 GB DDR4 RAM. The three-dimensional simulation domain had periodic boundary conditions in all directions. The computational grid had cells of equal size. The number of cells and order of finite element $p$ were varied.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.47\linewidth}
		\includegraphics[width=\linewidth]{figures/sl_vs_plb_1node_3d.pdf}
		\subcaption{NATriuM's performance with semi-Lagrangian streaming 
		at different orders of finite element $p$ and grid sizes
		(the semi-Lagrangian streaming does not require a time integrator). \label{fig:perf_sl}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.47\linewidth}
		\includegraphics[width=\linewidth]{figures/sl_vs_plb_1node_3d_sedg.pdf}
		\subcaption{NATriuM's performance with discontinuous Galerkin streaming and a classical
		 fourth-order Runge-Kutta method
		 at different orders of finite element $p$ and grid sizes. \label{fig:perf_sedg}}
	\end{subfigure}
	\begin{subfigure}[c]{\linewidth}
		\includegraphics[width=\linewidth]{figures/barplot.pdf}
		\DIFdelbeginFL %DIFDELCMD < \subcaption{Performance of semi-Lagrangian LBM vs discontinuous Galerkin LBM 
%DIFDELCMD < 		with different time integrators for a fixed problem size of $110\, 592$ and second-order
%DIFDELCMD < 		finite elements. From left to right: semi-Lagrangian streaming; explicit Runge-Kutta methods: 
%DIFDELCMD < 		explicit Euler, third-order Runge-Kutta, classical fourth-order Runge-Kutta; implicit Runge-Kutta
%DIFDELCMD < 		methods: implicit Euler, implicit midpoint, Crank-Nicolson, singly-diagonally implicit Runge-Kutta;
%DIFDELCMD < 		exponential integrator \cite{Uga.2013}; embedded Runge-Kutta method: Cash-Karp.
%DIFDELCMD < 		\label{fig:perf_integrators}}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \subcaption{Performance of semi-Lagrangian LBM vs discontinuous Galerkin LBM 
		with different time integrators for a fixed problem size of $110\, 592$ and second-order
		finite elements. From left to right: semi-Lagrangian streaming; (explicit Runge-Kutta methods:) 
		explicit Euler, third-order Runge-Kutta, classical fourth-order Runge-Kutta; (implicit Runge-Kutta
		methods:) implicit Euler, implicit midpoint, Crank-Nicolson, singly-diagonally implicit Runge-Kutta;
		exponential integrator \cite{Uga.2013}; (embedded Runge-Kutta method:) Cash-Karp.
		\label{fig:perf_integrators}}
	\DIFaddendFL \end{subfigure}
	\caption{\label{fig:perf}Computational performance of NATriuM on a single compute node for different
	NATriuM modules. The simulations in this figure used either the semi-Lagrangian streaming step 
	or the discontinuous Galerkin streaming step. Nine different time integrators were used in combination
	with the discontinuous Galerkin scheme. The CFL and Mach number in these \DIFdelbeginFL \DIFdelFL{simulation }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{simulations }\DIFaddendFL were $CFL=0.4$ and $Ma=0.1,$
	respectively.}
\end{figure}

Figures \ref{fig:perf_sl} and \ref{fig:perf_sedg} depict the performance of NATriuM when used with a semi-Lagrangian streaming step and the discontinuous Galerkin streaming step that was introduced by Min and Lee \cite{Min.2011}. The semi-Lagrangian \DIFdelbegin \DIFdel{streaming step is }\DIFdelend \DIFaddbegin \DIFadd{lattice Boltzmann method reached $10$ MLUPS, which was }\DIFaddend $5$ to $10$ times faster than the discontinuous Galerkin scheme with classical Runge-Kutta time stepping. In addition, the performance \DIFdelbegin \DIFdel{does }\DIFdelend \DIFaddbegin \DIFadd{did }\DIFaddend not decrease as quickly \DIFdelbegin \DIFdel{when }\DIFdelend \DIFaddbegin \DIFadd{for growing }\DIFaddend $p$\DIFdelbegin \DIFdel{grows}\DIFdelend . As expected, the number of grid point updates per second \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend significantly lower than for standard LBM codes. \DIFdelbegin \DIFdel{For }\DIFdelend \DIFaddbegin \DIFadd{By }\DIFaddend comparison, Palabos reached 140 MLUPS on the same hardware. The degraded performance of OLBMs is \DIFdelbegin \DIFdel{clear from their formulation and }\DIFdelend \DIFaddbegin \DIFadd{due to increased code complexity and memory access patterns and }\DIFaddend might still be acceptable \DIFdelbegin \DIFdel{, }\DIFdelend because they can reduce the number of required grid points by multiple orders of magnitude, cf. the next two Sections.

Figure \ref{fig:perf_integrators} compares the semi-Lagrangian LBM and the discontinuous Galerkin LBM with various different time integrators, including explicit, implicit, and embedded Runge-Kutta methods as well as an explicit integrator. Embedded Runge-Kutta methods \DIFdelbegin \DIFdel{like }\DIFdelend \DIFaddbegin \DIFadd{such as }\DIFaddend the Cash-Karp method adapt the time step size dynamically to the problem during the course of the simulation. When using embedded Runge-Kutta methods, NATriuM automatically adjusts the relaxation time $\tau$ to the current time step size. The exponential integrator generally follows a previous publication \cite{Uga.2013}. 

The performance of NATriuM with a discontinuous Galerkin time stepping varied strongly with respect to the time integrator. The semi-Lagrangian LBM was faster than the discontinuous Galerkin LBM with all time integrators. It should be noted that the performance of implicit time integrators depends strongly on the time step size. In spite of their low number of less than one million lattice updates per second, they can still be useful when the problem allows large CFL numbers $$CFL:=\frac{\min\delta_{x}}{\max ||e_{\alpha}|| (p+1)^2 \delta_{t}}.$$ Prominent examples \DIFdelbegin \DIFdel{for }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend such problems are low-$Re$ flows, stationary flows and heterogeneously refined-grids. In these situations, the implicit integrators, as well as the semi-Lagrangian streaming step, can exploit the fact that they do not incorporate a strict CFL constraint, cf. Section \ref{sec:Unstructured}. 

In sum, this performance study advertises the use of our newly developed semi-Lagrangian streaming step within NATriuM. Moreover, it demonstrates \DIFdelbegin \DIFdel{that }\DIFdelend \DIFaddbegin \DIFadd{how the extensible code structure can boost the incorporation of }\DIFaddend new modules, e.g. advection operators and time integrators\DIFdelbegin \DIFdel{, can be easily incorporated into the existing modular code structure}\DIFdelend .

%
%
%\subsection{Extensibility: Incorporation of time integrators}
%
%\begin{figure}[htbp]
% 	\centering
%	\includegraphics[width=0.9\linewidth]{tgv_err_vs_cfl.png}
%	\includegraphics[width=0.9\linewidth]{tgv_err_vs_runtime.png}
% 	\caption{\label{fig:TGV_integrators} Left: Errors on velocity for the Taylor-Green vortex simulations with different time integrators versus the CFL numbers. A regular grid was used with $N=2,\ p=7,\ Ma=0.01$. Right: Errors versus total runtime on a single core for the same simulations. Implicit Runge-Kutta methods (blue lines): Implicit Euler(I. Euler), Implicit Midpoint (Midpoint), Crank-Nicolson (Crank), two-stage singly-diagonally implicit Runge-Kutta (SDIRK). Explicit Runge-Kutta methods (dark red lines): Explicit Euler (E. Euler),  classical fourth order explicit Runge-Kutta (RK4), third-order explicit Runge-Kutta (RK3). Embedded Runge-Kutta methods (green points): Heun-Euler (Heun-Eu), Bogacki-Shampine (Bo-Sha), Dorman-Price (Do-Pri), Runge-Kutta-Fehlberg (Fehlberg), Cash-Karp (Ca-Ka). Exponential integrator (black line).}
%\end{figure}
%
%To prove the extensibility, the previous simulations were performed with TBD different time integrators. When NATriuM already had the classical Runge-Kutta method, the Theta method and an exponential integrator implemented, deal.II was extended by a number of explicit, implicit, and embedded Runge-Kutta methods.  Incorporation into NATriuM was a simple task. An interface class was built and made available to the user via the configuration file. 
%
%Figure \ref{fig:TGV_integrators} shows the temporal errors in the simulation of the Taylor-Green vortex. The spatial resolution and Mach number were constant: $p=7, N=2, Ma=0.01.$ The $CFL$ number was varied. The embedded Runge Kutta methods were configured to tolerate errors between $10^{-8}$ and $10^{-7}$ per time step. When the error estimate was smaller than $10^{-8}$, the time step was increased by a factor of $TBD.$ When it was greater than $10^{-7}$, the time step was decreased by a factor of $TBD.$  Blue, dark red, and black lines represent the implicit, explicit, and exponential integrators, respectively; green symbols represent the embedded Runge-Kutta methods. 
%
%Most of the integrators yielded second-order convergence, even the higher-order explicit Runge-Kutta methods and the exponential integrator. This stems from the second-order splitting into advection and streaming (see \ref{sec:Appendix}). With first-order accurate integrators, the convergence broke down. In spite of the theoretical predictions, the singly-diagonally implicit Runge-Kutta method (SDIRK) increased the temporal order. This comes as a surprise and needs further investigation. 
%
%Explicit integration restricts the time step to lower CFL numbers, while implicit integration allows for CFL numbers up to $100$. Still, a runtime comparison favors the third-order and fourth-order Runge Kutta methods and the exponential integrator (see Figure \ref{fig:TGV_runtimes}). However, implicit integration is a valuable alternative when it comes to irregular grids and steady flows. This was already demonstrated in previous work. 
%



\section{High-Order Discretization}
\subsection{Description}
Both advection operators that are currently implemented in NATriuM can be used with an arbitrary order $p$ of finite elements, allowing the spatial convergence order to be increased. This is a useful feature to attain highly accurate results. A high spatial order can also facilitate a drastic reduction of grid points, while retaining the same accuracy.

\subsection{Results and Discussion}
%The flow domain $D=[0,1]^2$ had cells of equal size and periodic boundaries along the horizontal axis. The upper wall had a fixed velocity $u_{\mathrm{char}} = 0.05/\sqrt{3},$ while the lower was kept stationary. The Reynolds, Mach and CFL number were set to $Re=2000,$ $Ma=0.05,$ and $CFL=0.4$, respectively.  The analytic solution is given by \cite{Min.2011}
%\begin{equation*}
%u(y,t) = u_{\mathrm{char}} \frac{y}{l_{\mathrm{char}}} + \sum_{m=1}^{\infty} \frac{2u_{\mathrm{char}} (-1)^{m}}{\lambda_{m} l_{\mathrm{char}}}
%\exp \left( -\nu \lambda_{m}^2 t \right) \sin \left( \lambda_{m} y \right)
%\end{equation*}
%with $\lambda_{m} = \frac{m \pi}{l_{\mathrm{char}}},\ m \in \mathbb{N}. $
%The order of finite elements $p\in 1,\dots, 10$ and the number of grid points were varied. The velocities were compared to the reference solution at $t=t_{\max}.$
%Palabos was used with three different wall boundary conditions, all known to be second-order accurate: first, the one by Zou and He \cite{Zou.1997}, second, the interpolated scheme by Skordos \cite{Skordos.1993}, and third, the regularized scheme by Latt \cite{Latt.2008}.
%
%\begin{figure}[htpb]
%\centering
%\includegraphics[width=\linewidth]{couette_acoustic_err_vs_runtime_supnorm.eps}
%\caption{Errors versus runtime for the simulations of the unsteady Couette flow with NATriuM and Palabos. NATriuM was used with cells of equal size with grid sizes of $4^1, 4^2$ and $4^3$ cells and varying orders of finite elements, $p=1,\dots,10.$ Palabos used grid sizes ranging from $4^2$ to $4^9$ lattice nodes.
%\label{fig:couette_runtime}}
%\end{figure}
%The results are given in appendix \ref{sec: Couette flow}. The convergence in Palabos was of first order for all three boundary conditions. This is due to the acoustic scaling ($Ma = 0.05 = \mathrm{const}$). When used with a diffusive scaling ($Ma \sim \delta_{x}$), Palabos achieves second-order accuracy, at the cost of quadratic runtime \cite{Latt.2008}. NATriuM retained the exponential convergence of the spatial discretization. This had a huge effect: While Palabos only reached an error of $9.53\e{-4}$ with $262\,144$ grid points, NATriuM reached $9.22\e{-6}$ with only $7\,744$ grid points. These two simulations had a comparable total runtime with substantially better results for NATriuM. This is supported by Figure \ref{fig:couette_runtime} which shows the errors versus runtime of all simulations. The SEDG-LBM clearly dominates the standard LBM when a high accuracy is desired.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.55\textwidth]{figures/pconvergence.pdf}
  \caption{Convergence of the errors for the two-dimensional Taylor-Green vortex.
   $Ma:=10^{-4}$ and $CFL=0.1$ were kept constant and the order of finite elements and grid spacing were varied.\label{fig:convergence}}
\end{figure}

The Taylor-Green decaying vortex is a standard benchmark to test a solver's convergence. In the doubly-periodic domain $[0,2\pi]^2,$ the incompressible solution is given by
\begin{align*}
u_{1}^{\mathrm{ref}}(x,y,t) = \sin(x) \cos(y) \exp(-2\nu t),\\
u_{2}^{\mathrm{ref}}(x,y,t) =- \cos(x) \sin(y) \exp(-2\nu t),
\end{align*}
where $\nu$ is the kinematic viscosity. 
To demonstrate the convergence, the spatial resolution was varied at constant Reynolds, Mach and CFL number $Re = \frac{UL}{\nu} = \frac{2\pi}{\nu} := 10, Ma = 10^{-4}, CFL=0.1,$ respectively.
The \DIFdelbegin \DIFdel{Errors }\DIFdelend \DIFaddbegin \DIFadd{errors }\DIFaddend $||u-u^{\mathrm{ref}}||_2$ were measured at \DIFdelbegin \DIFdel{$t_{\mathrm{max}}:=TBD.$ 
}\DIFdelend \DIFaddbegin \DIFadd{$t_{\mathrm{max}}:=-\ln(0.1)/(2\nu),$ i.e. the eddy-turnover time.
}\DIFaddend 

Figure \ref{fig:convergence} shows the convergence with respect to the order $p$ of shape functions and cell width $\delta_x$. The errors decay with increasing $p$ and \DIFaddbegin \DIFadd{with }\DIFaddend decreasing $\delta_x$\DIFdelbegin \DIFdel{, respectively}\DIFdelend . The dependence on $p$ is exponential, as in previous studies \cite{Min.2011, Kramer.2016c}.

This result underlines the theoretical finding that the spatial convergence order of OLBMs depends solely on the discretization of the streaming step. The advection operators implemented in NATriuM support high-order spatial convergence. As demonstrated by the above result, this convergence property can give highly accurate results even on coarse grids. 



\section{Irregular Grids}
\label{sec:Unstructured}
\subsection{Description}
NATriuM can handle arbitrary quadrilateral and hexagonal grids in 2D and 3D, respectively. It can both \DIFdelbegin \DIFdel{read in grids from }\DIFdelend \DIFaddbegin \DIFadd{import grids of }\DIFaddend different formats and generate (or manipulate) them \DIFdelbegin \DIFdel{by }\DIFdelend \DIFaddbegin \DIFadd{using }\DIFaddend deal.II's grid generating functions. Internally, the grids are stored as distributed octrees via the library p4est, yielding cache-efficient and highly-parallel data structures. NATriuM writes out checkpoints during a simulation. When restarting the simulation from a checkpoint with a differently refined grid, it automatically interpolates the distribution functions from the previous grid to the present grid. The possibility to use non-Cartesian grids is one of the major advantages of NATriuM over other lattice Boltzmann tools.

\subsection{Results and Discussion}
\begin{table}
\centering
\caption{Sinusoidal shear flow configurations that were studied.\label{tab:sine_config}}
\begin{tabular}{c c c}
\hline 
\rule[-1ex]{0pt}{0.0ex} Conf. & $L$ & $a$ \\ 
\cmidrule{1-3}
\rule[-1ex]{0pt}{0.0ex} 1 & $5$ & $0.05$ \\ 
\rule[-1ex]{0pt}{0.0ex} 2 & $1$ & $0.05$ \\ 
\rule[-1ex]{0pt}{0.0ex} 3 & $5$ & $0.1$ \\ 
\rule[-1ex]{0pt}{0.0ex} 4 & $1$ & $0.1$ \\ 
%\rule[-1ex]{0pt}{0.0ex} 5 & $1$ & $0.29$ \\ 
\hline 
\end{tabular} 
\end{table}


\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{figures/grid-cfg1.png}
	\caption{ Computation grid for sinusoidal shear flow with $4 \cdot 4^3$ cells. The depicted sinusoidal geometry has an average height of $\bar{h} = 0.3,$ an amplitude of $a=0.1$ and a wavelength of $L=1,$ i.e. Conf. 4 in Table \ref{tab:sine_config}.
\label{fig:sine_grid}}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[height=5cm]{figures/psi_vs_ref_plb.pdf}
		\subcaption{Shear flow factors versus runtime for Palabos (standard LBM) simulations in sinusoidal geometries.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.56\linewidth}
		\includegraphics[height=5cm]{figures/psi_vs_ref_rk.pdf}
		\DIFdelbeginFL %DIFDELCMD < \subcaption{Shear flow factors versus runtime for NATriuM (OLBM) simulations in sinusoidal geometries.}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \subcaption{Shear flow factors versus runtime for NATriuM (OLBM) simulations in sinusoidal geometries using 
			an explicit fourth-order Runge-Kutta method with $CFL=0.4$}
	\DIFaddendFL \end{subfigure}
	\DIFaddbeginFL \begin{subfigure}[t]{0.56\linewidth}
		\includegraphics[height=5cm]{figures/psi_vs_ref_cn.pdf}
		\subcaption{Shear flow factors versus runtime for NATriuM(OLBM) simulations using 
			an implicit time integrator, the Crank-Nicolson method with $CFL=100.$ 
			The fastest simulations took only a few seconds and gave very accurate 
			results for all configurations. \label{fig:sine_runtime_implicit}}
	\end{subfigure}
\DIFaddendFL \caption{Shear flow factors for different resolutions \DIFaddbeginFL \DIFaddFL{and orders of finite elements}\DIFaddendFL . The four configurations \DIFaddbeginFL \DIFaddFL{Conf. 1-4 }\DIFaddendFL are specified in Table \ref{tab:sine_config}. Results are compared to the analytic results by Letalleur \cite{Letalleur.2002}.
%DIF < While Palabos required high resolutions to obtain satisfactory results (highlighted), NATriuM was already accurate for the smallest grid size. This possibility to use a coarse grid rendered NATriuM two orders of magnitude faster than Palabos for the given application.
\label{fig:sine_runtime}}
\end{figure}

\DIFdelbegin %DIFDELCMD < \begin{figure}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=0.56\linewidth]{figures/psi_vs_ref_cn.pdf}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
%DIF < Left: Convergence of the flow factors in NATriuM for Configuration 5 with a stronger stop condition:  $| \Psi_{x}^{(i)}-\Psi_{x}^{(i-100)}| < 10^{-8}.$ $Ma$ was varied from $10^{-4}$ to $10^{-1}$. The grid had $4^2$ cells and the order of finite elements was $p=4.$ 
\DIFdelFL{Shear flow factors for simulations with NATriuM using an implicit time integrator, the Crank-Nicolson method with $CFL=100.$ The fastest simulations took only a few seconds and gave very accurate results for all configurations.
}%DIFDELCMD < \label{fig:sine_runtime_implicit}%%%
}
%DIFAUXCMD
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend To assess the benefits of arbitrary quadrilateral grids over regular lattices, both NATriuM and the standard LBM library Palabos \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Palabos} }%DIFAUXCMD
}\DIFdelend were used to simulate shear flows in sinusoidal geometries. These flows were simulated previously by Al-Zoubi and Brenner \cite{AlZoubi.2008}. The upper wall was one period of a sine wave with wavelength $L$. The average height was $\bar{h} = 0.3$ and the amplitude $a$, making a flow domain 
$$D_{\sin} := \{(x,y):\ 0 \leq x < 2\pi,\ 0 \leq y \leq \bar{h} + a\sin(2\pi x/L) \},$$ 
cf. Figure \ref{fig:sine_grid}.
The curved wall was kept stationary, while the lower moved with $u_w = 0.1,$ with periodic boundaries along the $x$-axis.
Four different channel configurations \DIFaddbegin \DIFadd{Conf. 1 - 4 }\DIFaddend were simulated (cf. Table \ref{tab:sine_config}) to evaluate the shear flow factors
\begin{equation*}
\Psi_{x} = \frac{ \sqrt{2}\cdot\bar{h}}{a} \left( 2 \cdot \frac{u_w - \bar{u}_x}{u_w} -1 \right).
\end{equation*}
For long channels, inertial effects can be disregarded to give an analytic expression for the shear flow factor \cite{Letalleur.2002}
\begin{equation*}
\Psi_{x} = \frac{3 \sqrt{2} \frac{a}{\bar{h}}}{2+\left( \frac{a}{\bar{h}} \right)^{2}}.
\end{equation*}
The simulations were stopped when the flow factor converged below a threshold of $| \Psi_{x}^{(i)}-\Psi_{x}^{(i-100)}| < 10^{-6},$ where $i$ denotes the iteration number. The Reynolds and Mach number were set to $1$ and $0.01,$ respectively. 

For Palabos, the domain was $[0,L] \times [0,\bar{h}+a]$ with a regular grid. The flat wall was implemented with the regularized BC \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Latt.2008}}%DIFAUXCMD
}\DIFaddend , while the curved wall and the solid part were composed of simple bounce-back nodes. The number of grid points per unit square was $4^N,\ N = 5, \dots ,10.$ An average over all bulk nodes (boundaries excluded) was used to determine $\bar{u}_x$.

For NATriuM, the discontinuous Galerkin scheme was used \DIFdelbegin \DIFdel{with }\DIFdelend on a grid with $4\cdot 4^N$ cells, $N \in \{2,3,4\}.$ The order of finite elements $p \in \{2,4,6,8\}$ was also varied. A classical fourth-order Runge-Kutta method was used for time integration, with a CFL number of $0.4.$ 


It is expected that\DIFaddbegin \DIFadd{, }\DIFaddend in long channels\DIFaddbegin \DIFadd{, }\DIFaddend the shear flow factors approach the analytic solution. In short channels, a vortex may form inside the bulge. This effect decreases the throughput, leading to an increase of shear flow factors. Figure \ref{fig:sine_runtime} shows the shear flow factors versus runtime for simulations with different spatial resolutions. 

With Palabos, only for the highest resolution \DIFaddbegin \DIFadd{did }\DIFaddend the shear flow factors \DIFdelbegin \DIFdel{agreed }\DIFdelend \DIFaddbegin \DIFadd{agree }\DIFaddend with the analytic solution. The required runtime to get acceptable results was roughly $10^3\,\mathrm{s}.$ With NATriuM, however, \DIFdelbegin \DIFdel{already }\DIFdelend \DIFaddbegin \DIFadd{even }\DIFaddend the smallest discretization delivered accurate results. The sufficiency of a coarse, irregular grid lowered the required runtime by two orders of magnitude compared to Palabos, demonstrating the advantage of unstructured grids. NATriuM could \DIFdelbegin \DIFdel{even }\DIFdelend \DIFaddbegin \DIFadd{further }\DIFaddend improve over these results by using an implicit time integrator:
Figure \ref{fig:sine_runtime_implicit} (right) shows the results for the same simulation setup with Crank-Nicolson time stepping and $CFL=100.$ Due to the large time step, it was possible to be even three orders of magnitude faster than Palabos while still giving accurate results. \DIFaddbegin \DIFadd{The advantage of implicit over explicit OLBMs for steady flows accords with recent work by Li and Luo \mbox{%DIFAUXCMD
\cite{Li.2016b}}%DIFAUXCMD
.
}\DIFaddend 

In sum, NATriuM was much more efficient than Palabos \DIFdelbegin \DIFdel{, }\DIFdelend due to the use of geometry-adaptive quadrilateral grids and implicit time integration. This result illustrates the usefulness of OLBMs. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend advantages of irregular \DIFdelbegin \DIFdel{lattices }\DIFdelend \DIFaddbegin \DIFadd{grids }\DIFaddend may well been \DIFdelbegin \DIFdel{transferred }\DIFdelend \DIFaddbegin \DIFadd{transferable }\DIFaddend to other flows in complex-shaped domains.


\section{Dimension-Independent Programming}
\DIFaddbegin \label{sec:dimension-independent}
\DIFaddend \subsection{Description}
NATriuM supports simulations in 2D and 3D that are largely based on the same programming code. The massive use of C++ templates facilitates dimension-independent programming, where most classes take the dimension as a template parameter. Deal.II uses the same concept\DIFdelbegin \DIFdel{that }\DIFdelend \DIFaddbegin \DIFadd{, which }\DIFaddend leads to a significant reduction of code. Most functionality in NATriuM is easily transferred from 2D to 3D, preventing duplicate code and obviating bugs.  

\subsection{Results and Discussion}
To support this feature by tangible results, we present the simulation of a three-dimensional turbulent channel flow. The computational domain $D=[0,2\pi]\times[0,1]\times[0,2\pi/3]$ was divided into $32\times 32\times 32$ cells that were refined at the solid walls (cf. \DIFdelbegin \DIFdel{Figure }\DIFdelend \DIFaddbegin \DIFadd{Fig. }\DIFaddend \ref{fig:turb_grid} ). The flow was driven by an external force with \DIFdelbegin \DIFdel{a prescribed $Re_{\tau} = 180$}\DIFdelend \DIFaddbegin \DIFadd{$Re_{\tau} = 140$}\DIFaddend . Periodic boundary conditions were used into the $x$ and $z$ directions\DIFdelbegin \DIFdel{and the flow was initialized with a turbulent profile}\DIFdelend .

The collision step was performed using the BGK scheme. The streaming step was performed using a spectral element discontinuous Galerkin scheme and an exponential integrator with $CFL=2.0$ and $Ma=0.05.$ After an equilibration ($400\, 000$ time steps), the turbulence statistics were averaged over every tenth of $430\, 000$ time steps \DIFdelbegin \DIFdel{, }\DIFdelend and over the homogeneous directions. %The actual Reynolds number was $Re_{\tau} = 140.$


\DIFaddbegin \DIFadd{Unfavorable initial conditions were used to check the robustness of the implementation, in combination with a short but still sufficient time period for calculating the statistical averages. A minimum channel configuration was used instead of the typically larger domain size, to demonstrate the stability of the present OLBM. (Notably, the turbulent simulations ran stable even on a very coarse grid with $8\times 8 \times8$ cells). The following results were obtained on a grid with $32 \times 32 \times 32$ cells. 
}



\DIFaddend \begin{figure}[htbp]
	\centering
		\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}[t]{\textwidth}
%DIFDELCMD < 		\centering
%DIFDELCMD < 		%%%
\DIFdelendFL \includegraphics[width=0.7\linewidth]{figures/make_grid_figure.pdf}
		\subcaption{Irregular computational grid for simulations of the turbulent channel flow. 
		The plot shows one slice of cells from the lower wall ($y=0$) to the channel center ($y=0.5$). 
		The grid has $32\times 32\times 32$ cells. 	The cells are equidistant in the $x$- and $z$-directions.
		\label{fig:turb_grid}}
\DIFdelbeginFL %DIFDELCMD < \end{subfigure}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \end{figure}
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \begin{subfigure}[t]{0.42\textwidth}
%DIFDELCMD < 		%%%
\DIFdelend \DIFaddbegin \begin{figure}		
	\DIFaddendFL \centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\linewidth]{figures/turb_u.pdf}
%DIFDELCMD < 		\subcaption{Average streamwise velocities in wall units. The logarithmic law of the wall 
%DIFDELCMD < 		($\langle u^{+} \rangle=0.41 \cdot \ln(y^{+}) + 5.5$) and $y^{+}$ 
%DIFDELCMD < 		are shown as a guide to the eye. 
%DIFDELCMD < 		\label{fig:turb_u}}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[height=6cm]{figures/turb_u.pdf}
		\subcaption{Average streamwise velocities in wall units 
		compared to \cite{Moser.1999}.
		In the reference, the wall Reynolds number was $Re_\tau = 180$.
		The logarithmic law of the wall ($u_{1}^{+} =0.41 \cdot \ln(y^{+}) + 5.5$) and $y^{+}$ 
		are shown as a guide to the eye. 
		\label{fig:turb_u}}
	\DIFaddendFL \end{subfigure}
	\hfill
	\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}[t]{0.42\textwidth}
%DIFDELCMD < 		\centering
%DIFDELCMD < 		\includegraphics[width=\linewidth]{figures/turb_tau.pdf}
%DIFDELCMD < 		\subcaption{Average stresses in wall units. The Reynolds stresses and viscous are defined as
%DIFDELCMD < 		$-\rho \langle u'^{+} v'^{+} \rangle$ and $d\langle u \rangle /dy,$ respectively. 
%DIFDELCMD < 		\label{fig:turb_tau}}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[height=6cm]{figures/turb_prod.pdf}
		\subcaption{Average production of kinetic energy in wall units 
		compared to \cite{Moser.1999}.
		In the reference, the wall Reynolds number was $Re_\tau = 180$.		
		The production is defined as $ TBD $.
		\label{fig:turb_prod}}
	\DIFaddendFL \end{subfigure}

	\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}[t]{0.42\textwidth}
%DIFDELCMD < 		%%%
\DIFdelendFL \DIFaddbeginFL \begin{subfigure}[t]{0.6\textwidth}
		\DIFaddendFL \centering
		\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\linewidth]{figures/turb_prod.pdf}
%DIFDELCMD < 		\subcaption{Average production of kinetic energy in wall units. 
%DIFDELCMD < 		The production is defined as $ TBD $.
%DIFDELCMD < 		\label{fig:turb_prod}}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[height=6cm]{figures/turb_u_poi.pdf}
		\subcaption{Average streamwise velocities, compared to \cite{Tsukahara.2014}. 
		In the reference, the wall Reynolds number was $Re_\tau = 150$.
		%Average stresses in wall units. The Reynolds and viscous stresses are defined as
		%$-\rho \langle u'^{+} v'^{+} \rangle$ and $d\langle u \rangle /dy,$ respectively. 
		\label{fig:turb_tau}}
	\DIFaddendFL \end{subfigure}
\DIFaddbeginFL 

	\DIFaddendFL \caption{\DIFdelbeginFL \DIFdelFL{Simulation of a }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Results for the }\DIFaddendFL turbulent channel flow at \DIFdelbeginFL \DIFdelFL{$Re_\tau = 180$. The results are }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$Re_\tau = 140$, 
	}\DIFaddendFL compared to \DIFdelbeginFL \DIFdelFL{the reference by Moser et al. }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{references }\DIFaddendFL \cite{Moser.1999} \DIFaddbeginFL \DIFaddFL{and \mbox{%DIFAUXCMD
\cite{Tsukahara.2014}}%DIFAUXCMD
}\DIFaddendFL .  \label{fig:turbulent}}
\end{figure}

Figure \ref{fig:turbulent} shows some \DIFdelbegin \DIFdel{statistics }\DIFdelend \DIFaddbegin \DIFadd{statistical averages }\DIFaddend of this simulation, compared to the \DIFdelbegin \DIFdel{reference }\DIFdelend \DIFaddbegin \DIFadd{references }\DIFaddend of Moser et al. \cite{Moser.1999} \DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{and Tsukahara et al. \mbox{%DIFAUXCMD
\cite{Tsukahara.2014} }%DIFAUXCMD
that were obtained at Reynolds numbers $Re_\tau = 180$ and $Re_\tau = 150,$ respectively. }\DIFaddend The average streamwise velocities (\DIFdelbegin \DIFdel{Figure }\DIFdelend \DIFaddbegin \DIFadd{Fig. }\DIFaddend \ref{fig:turb_u}) agreed well with \DIFdelbegin \DIFdel{the reference velocities }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Moser} }%DIFAUXCMD
}\DIFaddend and reproduced the viscous sublayer and the log-layer. \DIFdelbegin \DIFdel{The viscous and Reynolds stresses at the wall were also in close match (Figure \ref{fig:turb_tau}). The }\DIFdelend %DIF > The viscous and Reynolds stresses at the wall also matched closely (Fig. \ref{fig:turb_tau}). 
\DIFaddbegin \DIFadd{The }\DIFaddend production of turbulence kinetic energy (\DIFdelbegin \DIFdel{Figure \ref{fig:turb_prod}) was slightly underestimated at the peak, i. 
e. $y^{+} \approx 10,$ but accurate otherwise. Note that all statistics are given with respect to the actual $Re_\tau \approx 140,$ which explains the deviations in the center of the channel. (The considerably lower actual $Re_\tau$ is due to inaccurate initial conditions and a rather short equilibration.) While 
}\DIFdelend \DIFaddbegin \DIFadd{Fig. \ref{fig:turb_prod}) also matched the reference data \mbox{%DIFAUXCMD
\cite{Moser.1999}}%DIFAUXCMD
. 
While 
While }\DIFaddend the simulation by Moser et al. \cite{Moser.1999} used $128\times 129\times 128$ grid points, starting at $y^{+} = 0.053$, the present simulation used only $64\times 64\times 64$ grid points, starting at $y^{+}= 0.33.$ In spite of the low resolution\DIFdelbegin \DIFdel{and the inaccurate Reynolds number}\DIFdelend , the major characteristics of the turbulent channel flow were captured.


This result demonstrates that the use of irregular grids in the lattice Boltzmann method can dramatically reduce the number of grid points, cf. \cite{Chikatamarla.2013,Karlin.2014} for comparison. Furthermore, it underlines the capability of NATriuM to simulate three-dimensional flows on irregular grids using the lattice Boltzmann method.


\section{Conclusions}
This paper presents a survey of our new off-lattice Boltzmann solver NATriuM. NATriuM is a modular and parallel C++ code that facilitates the simulations of flows on irregular grids using the lattice Boltzmann method. The possibility of using arbitrary quadrilateral and hexagonal grids is achieved by replacing the streaming step by a more sophisticated procedure, such as a semi-Lagrangian or a discontinuous Galerkin streaming. As demonstrated in the simulations within this manuscript, the use of irregular grids as well as high-order spatial discretizations can be a decisive advantage, especially when it comes to modeling wall-bound flows. These features help to greatly reduce the number of required grid points, offsetting the additional cost that is introduced by the sophisticated streaming step. NATriuM's scalability and extensibility make it an ideal tool to explore the future prospects of off-lattice Boltzmann methods and their potential to study complex scientific and industrial flows.




\section{Acknowledgments}
The authors thank Natan Zawadzki for implementing the turbulent channel flow, Siamak Bayat for contributing to the 3D implementation, \DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{Vincent Frisch and Tobias Fraatz for assistance with the Palabos simulations, }\DIFaddend Thomas Brandes for helpful discussions about the efficiency and parallelization\DIFdelbegin \DIFdel{.
%DIF < The authors thank the Graduate Institute of the Bonn-Rhein-Sieg University of Applied Sciences for funding. (?)
}\DIFdelend \DIFaddbegin \DIFadd{, Anne Wegner for proofreading the manuscript, and Rudolf Berrendorf and Javed Razzaq for assistance with BRSU's WR cluster.
}\DIFaddend 


\appendix

%\section{Results for the Couette Flow}
%\label{sec:Couette}
%Tables \ref{tab:couette_errors_plb} and  \ref{tab:couette_errors_ntrm} show the maximum relative errors for Palabos and NATriuM, respectively. We use the supremum norm in the sense $||\cdot||_{\infty} := \max_{x \in D} |\cdot|_2$. 
%
%\begin{table}
%\small
%\centering
%\caption{Relative errors for the unsteady Couette flow simulations in Palabos. The refinement level $N$ was varied from $2$ to $9$ with $4^N$ grid points in the lattice. \label{tab:couette_errors_plb}}
%\begin{tabular}{c r | c c c}
%\hline
% &  & \multicolumn{3}{c}{$||u-u_{\mathrm{ref}}||_{\infty} / ||u_{\mathrm{ref}}||_{\infty}$} \\ 
% \hline
%$N$ & $\#$grid points & Regularized & Skordos & Zou, He \\ 
%\hline 
%$2$	&	$16$	&	$2.06\e{-1}$	&	$3.23\e{-1}$	&	$1.05\e{-1}$	\\
%$3$	&	$64$	&	$8.36\e{-2}$	&	$1.30\e{-1}$	&	$6.65\e{-2}$	\\
%$4$	&	$256$	&	$3.91\e{-2}$	&	$5.33\e{-2}$	&	$3.76\e{-2}$	\\
%$5$	&	$1\,024$	&	$1.73\e{-2}$	&	$2.11\e{-2}$	&	$2.15\e{-2}$	\\
%$6$	&	$4\,096$	&	$8.10\e{-3}$	&	$9.06\e{-3}$	&	$1.37\e{-2}$	\\
%$7$	&	$16\,384$	&	$3.92\e{-3}$	&	$4.16\e{-3}$	&	$9.92\e{-3}$	\\
%$8$	&	$65\,536$	&	$1.92\e{-3}$	&	$1.98\e{-3}$	&	$8.02\e{-3}$	\\
%$9$	&	$262\,144$	&	$9.53\e{-4}$	&	$9.67\e{-4}$	&	--	\\
%\hline 
%\end{tabular} 
%\end{table}
%
%\begin{table}
%\small
%\centering
%\caption{Relative errors for the unsteady Couette flow simulations in NATriuM. The numbers of cells were $4^N ,\ N=1,\dots,3.$
%The order of finite elements $p$ was varied from $1$ to $10$. The errors were defined as $||u-u_{\mathrm{ref}}||_{\infty} / ||u_{\mathrm{ref}}||_{\infty}$.
%\label{tab:couette_errors_ntrm}}
%\begin{tabular}{c | c c | c c | c c}
%\hline
% &  \multicolumn{2}{c}{$N=1$} & \multicolumn{2}{c}{$N=2$} & \multicolumn{2}{c}{$N=3$}\\ 
% \hline
%$p$ & $\#$grid points & error & $\#$grid points & error & $\#$grid points & error \\ 
%\hline 
%$1$	&	$16$	&	$1.53\e{-1}$	&	$64$	&	$2.22\e{-1}$	&	$256$	&	$3.24\e{-1}$	\\
%$2$	&	$36$	&	$6.63\e{-2}$	&	$144$	&	$8.54\e{-2}$	&	$576$	&	$7.98\e{-2}$	\\
%$3$	&	$64$	&	$3.95\e{-2}$	&	$256$	&	$6.03\e{-2}$	&	$1\,024$	&	$5.45\e{-3}$	\\
%$4$	&	$100$	&	$5.35\e{-2}$	&	$400$	&	$1.17\e{-2}$	&	$1\,600$	&	$8.42\e{-3}$	\\
%$5$	&	$144$	&	$4.25\e{-2}$	&	$576$	&	$1.60\e{-2}$	&	$2\,304$	&	$1.49\e{-3}$	\\
%$6$	&	$196$	&	$7.70\e{-3}$	&	$784$	&	$7.26\e{-3}$	&	$3\,136$	&	$3.78\e{-4}$	\\
%$7$	&	$256$	&	$1.64\e{-2}$	&	$1\,024$	&	$2.08\e{-3}$	&	$4\,096$	&	$1.04\e{-4}$	\\
%$8$	&	$324$	&	$1.49\e{-2}$	&	$1\,296$	&	$1.67\e{-3}$	&	$5\,184$	&	$4.19\e{-5}$	\\
%$9$	&	$400$	&	$4.26\e{-3}$	&	$1\,600$	&	$3.76\e{-4}$	&	$6\,400$	&	$1.18\e{-5}$	\\
%$10$	&	$484$	&	$2.55\e{-3}$	&	$1\,936$	&	$1.72\e{-4}$	&	$7\,744$	&	$9.22\e{-6}$	\\
%\hline 
%\end{tabular} 
%\end{table}


\section{Splitting the discrete Boltzmann equation into collision and advection step}
\label{sec:Appendix}
A pivotal prerequisite for NATriuM is that the solution of the discrete Boltzmann equation can be split into streaming and collision without affecting the spatial accuracy. To prove this \DIFdelbegin \DIFdel{preliminary}\DIFdelend \DIFaddbegin \DIFadd{prerequisite}\DIFaddend , we integrate the discrete Boltzmann equation \eqref{eq:DBE} along characteristics, giving
\begin{align*}
	f_{\alpha} (t+\delta_t, x+\delta_t e_{\alpha}) - f_{\alpha} (t,x)
		&= \int_{t}^{t+\delta_t} \Omega_{\alpha} (f) dt \\
		&= \frac{\delta_t}{2} \big[ \Omega_{\alpha} \big(f(t,x)\big) 
			+ \Omega_{\alpha} \big(f(t+\delta_t, x+\delta_t e_{\alpha}) \big) \big] 
			+ O \left( \delta_{t}^{2} \right).
\end{align*}
This implicit equation is transformed into an explicit equation by
$\bar{f}_{\alpha} := f_{\alpha} - \frac{\delta_t}{2} \Omega_{\alpha}(f):$
\begin{equation}
	\label{eq:transformed}
	\bar{f}_{\alpha} (t+\delta_t, x+\delta_t e_{\alpha}) - \bar{f}_{\alpha} (t,x)
		= \delta_t \Omega_{\alpha} \big( f(x,t) \big) + O \left( \delta_{t}^{2} \right).
\end{equation}

The final step is to express the collision operator in terms of $\bar{f}$. \DIFdelbegin \DIFdel{Therefor }\DIFdelend \DIFaddbegin \DIFadd{Therefore }\DIFaddend the latter is required to take the form
\begin{equation*}
	\Omega(f) = Rf + R^{\mathrm{eq}} (f),
\end{equation*}
where $R$ is linear and $R^{\mathrm{eq}}(\bar{f}) = R^{\mathrm{eq}}(f).$
This form includes the most important collision operators in the literature, most importantly the BGK model.
Rewriting 
\begin{align*}
	\Omega (f) 	= Rf + R^{\mathrm{eq}} (f) 
				= R \left( \bar{f} + \frac{\delta_t}{2}\Omega(f) \right) + R^{\mathrm{eq}} (\bar{f})
				= \Omega (\bar{f}) +  \frac{\delta_t}{2} R \Omega(f),
\end{align*}
we get
\begin{equation*}
	\Omega(f) = \left( I - \frac{\delta_t}{2} R \right)^{-1} \Omega(\bar{f}).
\end{equation*}
Inserting into Equation \eqref{eq:transformed} gives the difference equation
\begin{equation*}
	\bar{f}_{\alpha} (t+\delta_t, x+\delta_t e_{\alpha}) - \bar{f}_{\alpha}(t,x)
		= \delta_t \left( I-\frac{\delta_t}{2} R \right)^{-1} \Omega(\bar{f}) 
			+ O \left( \delta_{t}^{2} \right),
\end{equation*}
that is easily split into collision and advection;
and for the \DIFdelbegin \DIFdel{rest of the }\DIFdelend \DIFaddbegin \DIFadd{whole }\DIFaddend paper (except Equation \eqref{eq:DBE}) we redefine
\begin{align*}
	f &:= \bar{f}\\
	\mathrm{and} \quad
	\Omega &:= \left( I - \frac{\delta_t}{2} R \right)^{-1} \Omega.
\end{align*}
Note that for the BGK collision term, this transformation leads to the well-known $0.5$-shift in the relaxation time.
\DIFaddbegin 

\section{\DIFadd{Semi-Lagrangian Streaming Step}}
\label{sec:SL}
\DIFadd{The semi-Lagrangian streaming step is discussed in depth in a separate publication \mbox{%DIFAUXCMD
\cite{Kramer.2016b}}%DIFAUXCMD
.
It represents the distribution functions by a finite element instead of a point-wise approximation.
The finite element representation uses Lagrangian tensor-product shape functions $\tilde{\psi}_i$ that interpolate the function values at the Gau}{\DIFadd{\ss}}\DIFadd{-Lobatto-Legendre support points \mbox{%DIFAUXCMD
\cite{Marsden.2008} }%DIFAUXCMD
$\tilde{x}_i,\ i=1,\dots,(p+1)^{\mathrm{dim}},$ of a unit cell $\tilde{D} := [0,1]^\mathrm{dim},$ where $p$ is the polynomial order.
The shape functions and support points are transferred to an arbitrary quadrilateral grid cell $D_c$ using a multilinear mapping function $M_c: D_c \rightarrow \tilde{D},$ yielding $x_{ci} := M_c^{-1} (\tilde{x}_i)$ and 
	}\begin{equation*}
		\DIFadd{\psi_{ci}(x) := \left\{
			\begin{array}{l l}
				\tilde{\psi}_i ( M_c(x)),\ &\mathrm{if}\ x \in D_c\\
				0,\ & \mathrm{else}.
			\end{array}
		\right.
	}\end{equation*}

\DIFadd{The distribution functions are approximated by 
	}\begin{equation*}
		\DIFadd{f_{\alpha}(x,t)\approx 
			\sum_{c=1}^{\mathrm{\#cells}} \quad 
			\sum_{i=1}^{{(p+1)}^{\mathrm{dim}}} \hat{f}_{\alpha c i}(t) \psi_{c i}(x),
	}\end{equation*}
\DIFadd{where $\hat{f}_{\alpha c i}$ are the degrees of freedom.
Applying this to the lattice Boltzmann equation at $x=x_{ci}$ gives the semi-Lagrangian streaming
}

	\begin{equation*}
		\DIFadd{\hat{f}_{\alpha c i}(t) = 
			\sum_{\zeta = 1}^{\mathrm{\#cells}} \quad  \sum_{j=1}^{{(p+1)}^{\mathrm{dim}}} 
			\hat{f}_{\alpha \zeta j}(t-\delta_t) \ 
			\psi_{\zeta j}(x_{c i} - \delta_t e_{\alpha}).
	\label{eq:sl_streaming}
	}\end{equation*}
\DIFaddend 


\section*{\refname}
\bibliographystyle{model1-num-names}
%\bibliographystyle{natbib}
\bibliography{natrium}

\end{document}


